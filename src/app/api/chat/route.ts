import { pinecone, PINECONE_INDEX } from "@/lib/pinecone";
import { openai } from "@ai-sdk/openai";
import { OpenAIEmbeddings } from "@langchain/openai";
import { streamText } from "ai";
import { PineconeStore } from "@langchain/pinecone";

// Allow streaming responses up to 30 seconds
export const maxDuration = 50;

export async function POST(req: Request) {
  const { messages, url } = await req.json();

  const embeddings = new OpenAIEmbeddings({
    model: "text-embedding-3-small",
  });

  const pineconeIndex = pinecone.Index(PINECONE_INDEX);

  const vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
    pineconeIndex,
    namespace: url,
  });

  const query = messages[messages.length - 1].content;
  const results = await vectorStore.similaritySearch(query);

  const result = await streamText({
    model: openai("gpt-3.5-turbo-0125"),
    // TODO:add message after completion
    onFinish: (e) => console.log(e.usage),
    temperature: 0,
    prompt: `You are a friendly AI assistant augmented with an Pinecone Vector Store.
      To help you answer the questions, a context will be provided and you can also use ${url}. This context is generated by querying the vector store with the user question.
      Answer the question at the end using only the information available in the context and chat history in markdown format.
      If the answer is not available in the chat history or context, do not answer the question and politely let the user know that you can only answer if the answer is available in context or the chat history.
      WORD COUNT:50 words(minimum)
      -------------
      Chat history:
      {chat_history}
      -------------
      Context:
      ${results.map((r) => r.pageContent).join("\n\n")}
      
      -------------

      Question: ${query}
      Helpful answer:`,
  });

  return result.toDataStreamResponse();
}
