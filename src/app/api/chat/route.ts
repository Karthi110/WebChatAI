import { pinecone } from "@/lib/pinecone";
import { openai } from "@ai-sdk/openai";
import { OpenAIEmbeddings } from "@langchain/openai";
import { streamText } from "ai";
import { PineconeStore } from "@langchain/pinecone";

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(req: Request) {
  const { messages, url } = await req.json();
  // await vectorizeData("https://ui.shadcn.com/docs");
  const embeddings = new OpenAIEmbeddings({
    model: "text-embedding-3-small",
  });

  const pineconeIndex = pinecone.Index("indexedurls");

  const vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
    pineconeIndex,
    namespace: url,
  });

  const query = messages[messages.length - 1].content;
  const results = await vectorStore.similaritySearch(query);
  console.log(query);

  const result = await streamText({
    model: openai("gpt-3.5-turbo-0125"),
    onFinish(event) {
      console.log("finished", event.responseMessages[0].content, url);
    },
    temperature: 0,
    messages: [
      {
        role: "system",
        content: `
          You are a friendly AI assistant augmented with a Pinecone Vector Store.
      To help you answer questions, a context will be provided. This context is generated by querying the vector store with the user question.
      You must answer the question using **only the information available** in the context and chat history. If the information is unavailable, do not answer the question, and politely let the user know that you can only answer if the answer is present in the provided context or chat history.
      Provide the output in below format using markdown.

          OUTPUT FORMAT:
          **{title}**
          {description}
          \n-----------------\n
          {code snippet if asked else ignore}
          {links in itallic  if asked else ignore}
          \n-----------------\n
          
        `,
      },
      {
        role: "user",
        content: `
    Use the following pieces of context (or previous conversation if needed) to answer the user's question in markdown format. 

If you don't know the answer, just say that you don't know, don't try to make up an answer.
        
\n----------------\n
    
    \n----------------\n
    PREVIOUS CONVERSATION:
    
    \n----------------\n
    CONTEXT:
    ${results.map((r) => r.pageContent)}
    
    USER INPUT: ${query}
        `,
      },
    ],
  });

  return result.toDataStreamResponse();
}
